import logging
from typing import List, Dict, Any
from datetime import datetime
from clients.google_sheets_client import GoogleSheetsClient
from clients.x_client import XClient
from clients.linkedin_client import LinkedInClient
from clients.ai_client import AIClient
from models.database import db_manager
from config import PLATFORMS, IMPORTANCE_THRESHOLD, SCRAPER_CONFIG, NITTER_INSTANCES, TWITTER_CLIENT_PRIORITY

logger = logging.getLogger(__name__)

class PostCollector:
    def __init__(self):
        self.sheets_client = GoogleSheetsClient()
        self.x_client = None
        self.linkedin_client = None
        self.ai_client = AIClient()
        
        # åˆå§‹åŒ–ç¤¾äº¤åª’é«”å®¢æˆ¶ç«¯
        self._initialize_clients()
    
    def _initialize_clients(self):
        """åˆå§‹åŒ–ç¤¾äº¤åª’é«”å®¢æˆ¶ç«¯"""
        try:
            if PLATFORMS.get('twitter', {}).get('enabled'):
                self._initialize_twitter_client()
            
            if PLATFORMS.get('linkedin', {}).get('enabled'):
                self.linkedin_client = LinkedInClient()
                logger.info("LinkedIn client initialized")
                
        except Exception as e:
            logger.error(f"Error initializing social media clients: {e}")
            
    def _initialize_twitter_client(self):
        """æ ¹æ“šé…ç½®çš„å„ªå…ˆé †åºåˆå§‹åŒ– Twitter å®¢æˆ¶ç«¯"""
        logger.info(f"Twitter client priority order: {TWITTER_CLIENT_PRIORITY}")
        
        for client_type in TWITTER_CLIENT_PRIORITY:
            client_type = client_type.strip().lower()
            
            try:
                if client_type == "nitter":
                    if self._try_nitter_client():
                        return
                elif client_type == "scraper":
                    if self._try_scraper_client():
                        return
                elif client_type == "api":
                    if self._try_api_client():
                        return
                else:
                    logger.warning(f"Unknown client type: {client_type}")
            except Exception as e:
                logger.error(f"Failed to initialize {client_type} client: {e}")
                continue
        
        # å¦‚æœæ‰€æœ‰å®¢æˆ¶ç«¯éƒ½å¤±æ•—ï¼Œä½¿ç”¨ API ä½œç‚ºæœ€å¾Œå‚™æ¡ˆ
        logger.error("All Twitter clients failed, using API as last resort")
        self.x_client = XClient()
        logger.info("X (Twitter) API client initialized (emergency fallback)")
    
    def _try_nitter_client(self) -> bool:
        """å˜—è©¦åˆå§‹åŒ– Nitter å®¢æˆ¶ç«¯"""
        if not NITTER_INSTANCES:
            logger.info("No Nitter instances configured, skipping")
            return False
            
        from clients.nitter_client import NitterClient
        nitter_client = NitterClient()
        if nitter_client.test_connection():
            self.x_client = nitter_client
            logger.info("âœ“ X (Twitter) Nitter client initialized successfully")
            logger.info(f"Using {len(nitter_client.working_instances)} working Nitter instances")
            return True
        else:
            logger.warning("âœ— No working Nitter instances available")
            return False
    
    def _try_scraper_client(self) -> bool:
        """å˜—è©¦åˆå§‹åŒ– Scraper å®¢æˆ¶ç«¯"""
        if not SCRAPER_CONFIG.get('use_scraper', False):
            logger.info("Scraper not enabled in config, skipping")
            return False
            
        if not SCRAPER_CONFIG.get('accounts'):
            logger.warning("No scraper accounts configured, skipping")
            return False
            
        from clients.x_scraper_client import XScraperClientSync
        self.x_client = XScraperClientSync()
        logger.info("âœ“ X (Twitter) scraper client initialized successfully")
        logger.info(f"Using {len(SCRAPER_CONFIG.get('accounts', []))} scraper accounts")
        return True
    
    def _try_api_client(self) -> bool:
        """å˜—è©¦åˆå§‹åŒ– API å®¢æˆ¶ç«¯"""
        from config import X_API_BEARER_TOKEN
        if not X_API_BEARER_TOKEN:
            logger.warning("No X API bearer token configured, skipping")
            return False
            
        self.x_client = XClient()
        logger.info("âœ“ X (Twitter) API client initialized successfully")
        return True
    
    def collect_all_posts(self) -> Dict[str, Any]:
        """
        æ”¶é›†æ‰€æœ‰å¸³è™Ÿçš„è²¼æ–‡ä¸¦é€²è¡ŒAIåˆ†æ
        """
        results = {
            'total_accounts': 0,
            'total_posts_collected': 0,
            'total_posts_analyzed': 0,
            'important_posts': 0,
            'errors': [],
            'start_time': datetime.utcnow().isoformat(),
            'end_time': None
        }
        
        try:
            # 1. å¾Google Sheetsç²å–è¦è¿½è¹¤çš„å¸³è™Ÿåˆ—è¡¨
            accounts = self.sheets_client.get_accounts_to_track()
            results['total_accounts'] = len(accounts)
            
            if not accounts:
                logger.warning("No accounts to track found")
                return results
            
            # 2. åŒæ­¥å¸³è™Ÿåˆ°æ•¸æ“šåº«
            db_manager.save_accounts(accounts)
            
            # 3. ç²å–å·²å­˜åœ¨çš„è²¼æ–‡URLï¼Œç”¨æ–¼å»é‡
            existing_urls = set(self.sheets_client.get_existing_post_urls())
            
            # 4. æ”¶é›†æ‰€æœ‰å¹³å°çš„è²¼æ–‡
            all_posts = []
            
            for account in accounts:
                if not account.get('active', True):
                    continue
                
                platform = account['platform'].lower()
                username = account['username']
                
                try:
                    posts = self._collect_posts_for_account(platform, username)
                    
                    # å»é‡ï¼šéæ¿¾å·²å­˜åœ¨çš„è²¼æ–‡
                    new_posts = [post for post in posts if post.get('post_url') not in existing_urls]
                    
                    if new_posts:
                        all_posts.extend(new_posts)
                        logger.info(f"Collected {len(new_posts)} new posts from {platform}/@{username}")
                    else:
                        logger.info(f"No new posts from {platform}/@{username}")
                        
                except Exception as e:
                    error_msg = f"Error collecting posts from {platform}/@{username}: {e}"
                    logger.error(error_msg)
                    results['errors'].append(error_msg)
            
            results['total_posts_collected'] = len(all_posts)
            
            # 5. ä¿å­˜åŸå§‹è²¼æ–‡åˆ°æ•¸æ“šåº«
            if all_posts:
                db_manager.save_posts(all_posts)
            
            # 6. AIåˆ†æè²¼æ–‡
            if all_posts:
                analyzed_posts = self._analyze_posts(all_posts)
                results['total_posts_analyzed'] = len(analyzed_posts)
                
                # 7. çµ±è¨ˆé‡è¦è²¼æ–‡
                important_posts = [post for post in analyzed_posts 
                                 if post.get('importance_score', 0) >= IMPORTANCE_THRESHOLD]
                results['important_posts'] = len(important_posts)
                
                # 8. ä¿å­˜åˆ†æçµæœåˆ°æ•¸æ“šåº«
                if analyzed_posts:
                    db_manager.save_analyzed_posts(analyzed_posts)
                
                # 9. å°‡é‡è¦è²¼æ–‡å¯«å…¥Google Sheets
                if important_posts:
                    success = self.sheets_client.write_analyzed_posts(important_posts)
                    if not success:
                        results['errors'].append("Failed to write results to Google Sheets")
                
                # 10. å°‡æ‰€æœ‰åˆ†æéçš„è²¼æ–‡å¯«å…¥All Postså·¥ä½œè¡¨
                if analyzed_posts:
                    all_success = self.sheets_client.write_all_posts_with_scores(analyzed_posts)
                    if not all_success:
                        results['errors'].append("Failed to write all posts to Google Sheets")
                
                logger.info(f"Processing complete: {len(analyzed_posts)} posts analyzed, {len(important_posts)} important posts")
            
        except Exception as e:
            error_msg = f"Critical error in post collection: {e}"
            logger.error(error_msg)
            results['errors'].append(error_msg)
        
        finally:
            results['end_time'] = datetime.utcnow().isoformat()
        
        return results
    
    def _collect_posts_for_account(self, platform: str, username: str) -> List[Dict[str, Any]]:
        """ç‚ºå–®å€‹å¸³è™Ÿæ”¶é›†è²¼æ–‡"""
        posts = []
        
        try:
            if platform in ['twitter', 'x'] and self.x_client:
                # è¨˜éŒ„æ˜¯å¦ä½¿ç”¨çˆ¬èŸ²
                collection_method = 'scraper' if SCRAPER_CONFIG.get('use_scraper', False) else 'api'
                logger.info(f"Collecting posts for @{username} using {collection_method}")
                
                posts = self.x_client.get_user_tweets(username, days_back=1)
                
                # ç‚ºçˆ¬èŸ²æ”¶é›†çš„è²¼æ–‡æ·»åŠ æ¨™è¨˜
                if collection_method == 'scraper':
                    for post in posts:
                        post['collection_method'] = 'scraper'
                        
            elif platform == 'linkedin' and self.linkedin_client:
                posts = self.linkedin_client.get_user_posts(username, days_back=1)
            else:
                logger.warning(f"No client available for platform: {platform}")
            
            # ç‚ºæ¯å€‹è²¼æ–‡æ·»åŠ å¸³è™Ÿåˆ†é¡ä¿¡æ¯
            for post in posts:
                post['category'] = self._get_account_category(platform, username)
                
        except Exception as e:
            logger.error(f"Error collecting posts for {platform}/@{username}: {e}")
        
        return posts
    
    def _get_account_category(self, platform: str, username: str) -> str:
        """ç²å–å¸³è™Ÿåˆ†é¡"""
        try:
            accounts = db_manager.get_active_accounts(platform)
            for account in accounts:
                if account['username'] == username:
                    return account.get('category', 'general')
        except Exception as e:
            logger.error(f"Error getting account category: {e}")
        
        return 'general'
    
    def _analyze_posts(self, posts: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ä½¿ç”¨AIåˆ†æè²¼æ–‡"""
        try:
            logger.info(f"Starting AI analysis for {len(posts)} posts")
            analyzed_posts = self.ai_client.batch_analyze(posts)
            logger.info(f"AI analysis completed: {len(analyzed_posts)} posts analyzed")
            return analyzed_posts
        
        except Exception as e:
            logger.error(f"Error in AI analysis: {e}")
            return []
    
    def collect_posts_by_platform(self, platform: str) -> Dict[str, Any]:
        """æ”¶é›†ç‰¹å®šå¹³å°çš„è²¼æ–‡"""
        results = {
            'platform': platform,
            'posts_collected': 0,
            'posts_analyzed': 0,
            'important_posts': 0,
            'errors': []
        }
        
        try:
            # ç²å–è©²å¹³å°çš„æ´»èºå¸³è™Ÿ
            accounts = db_manager.get_active_accounts(platform)
            
            if not accounts:
                logger.warning(f"No active accounts found for platform: {platform}")
                return results
            
            all_posts = []
            existing_urls = set(self.sheets_client.get_existing_post_urls())
            
            for account in accounts:
                username = account['username']
                try:
                    posts = self._collect_posts_for_account(platform, username)
                    new_posts = [post for post in posts if post.get('post_url') not in existing_urls]
                    all_posts.extend(new_posts)
                    
                except Exception as e:
                    error_msg = f"Error collecting from {username}: {e}"
                    results['errors'].append(error_msg)
            
            results['posts_collected'] = len(all_posts)
            
            if all_posts:
                # ä¿å­˜åˆ°æ•¸æ“šåº«
                db_manager.save_posts(all_posts)
                
                # AIåˆ†æ
                analyzed_posts = self._analyze_posts(all_posts)
                results['posts_analyzed'] = len(analyzed_posts)
                
                # çµ±è¨ˆé‡è¦è²¼æ–‡
                important_posts = [post for post in analyzed_posts 
                                 if post.get('importance_score', 0) >= IMPORTANCE_THRESHOLD]
                results['important_posts'] = len(important_posts)
                
                # ä¿å­˜åˆ†æçµæœ
                if analyzed_posts:
                    db_manager.save_analyzed_posts(analyzed_posts)
                
                # å¯«å…¥Google Sheets
                if important_posts:
                    self.sheets_client.write_analyzed_posts(important_posts)
                
                # åŒæ™‚å¯«å…¥æ‰€æœ‰åˆ†æéçš„è²¼æ–‡åˆ°All Postså·¥ä½œè¡¨
                if analyzed_posts:
                    self.sheets_client.write_all_posts_with_scores(analyzed_posts)
        
        except Exception as e:
            error_msg = f"Error in platform collection for {platform}: {e}"
            logger.error(error_msg)
            results['errors'].append(error_msg)
        
        return results
    
    def manual_analyze_post(self, post_url: str) -> Dict[str, Any]:
        """æ‰‹å‹•åˆ†æå–®å€‹è²¼æ–‡"""
        result = {
            'success': False,
            'post_url': post_url,
            'analysis': None,
            'error': None
        }
        
        try:
            # é€™è£¡å¯ä»¥å¯¦ç¾æ ¹æ“šURLç²å–è²¼æ–‡å…§å®¹ä¸¦åˆ†æçš„é‚è¼¯
            # ç”±æ–¼APIé™åˆ¶ï¼Œæš«æ™‚è¿”å›ä½”ä½ç¬¦
            logger.info(f"Manual analysis requested for: {post_url}")
            result['success'] = True
            result['analysis'] = "Manual analysis feature to be implemented"
            
        except Exception as e:
            result['error'] = str(e)
            logger.error(f"Error in manual analysis: {e}")
        
        return result
    
    def get_collection_stats(self) -> Dict[str, Any]:
        """ç²å–æ”¶é›†çµ±è¨ˆä¿¡æ¯"""
        try:
            session = db_manager.get_session()
            
            from models.database import Post, AnalyzedPost
            from sqlalchemy import func
            
            # åŸºæœ¬çµ±è¨ˆ
            total_posts = session.query(Post).count()
            total_analyzed = session.query(AnalyzedPost).count()
            
            # æŒ‰å¹³å°çµ±è¨ˆ
            platform_stats = session.query(
                Post.platform,
                func.count(Post.id).label('count')
            ).group_by(Post.platform).all()
            
            # é‡è¦è²¼æ–‡çµ±è¨ˆ
            important_posts = session.query(AnalyzedPost).filter(
                AnalyzedPost.importance_score >= IMPORTANCE_THRESHOLD
            ).count()
            
            # ä»Šæ—¥çµ±è¨ˆ
            from datetime import date
            today = date.today()
            today_posts = session.query(Post).filter(
                func.date(Post.collected_at) == today
            ).count()
            
            # åŒæ™‚æŸ¥è©¢ posts å’Œ analyzed_posts è¡¨ï¼Œå–æœ€æ–°çš„
            last_post = session.query(Post).order_by(Post.collected_at.desc()).first()
            last_analyzed = session.query(AnalyzedPost).order_by(AnalyzedPost.collected_at.desc()).first()
            
            # æ¯”è¼ƒå…©å€‹è¡¨ï¼Œä½¿ç”¨æœ€æ–°çš„è¨˜éŒ„
            if last_post and last_analyzed:
                if last_post.collected_at > last_analyzed.collected_at:
                    last_collection = last_post.collected_at.isoformat()
                    logger.info(f"ğŸ“Š Using last collection from posts table: {last_collection}")
                else:
                    last_collection = last_analyzed.collected_at.isoformat()
                    logger.info(f"ğŸ“Š Using last collection from analyzed_posts table: {last_collection}")
            elif last_analyzed:
                last_collection = last_analyzed.collected_at.isoformat()
                logger.info(f"ğŸ“Š Only analyzed_posts has data: {last_collection}")
            elif last_post:
                last_collection = last_post.collected_at.isoformat()
                logger.info(f"ğŸ“Š Only posts table has data: {last_collection}")
            else:
                last_collection = None
                logger.warning("ğŸ“Š No data in either posts or analyzed_posts tables")
            
            # ç²å–æœ€æ–°è²¼æ–‡çš„ç™¼å¸ƒæ™‚é–“ï¼ˆç”¨æ–¼åƒè€ƒï¼‰
            latest_post_time = last_post.post_time.isoformat() if (last_post and last_post.post_time) else None
            
            session.close()
            
            return {
                'total_posts': total_posts,
                'total_analyzed': total_analyzed,
                'important_posts': important_posts,
                'today_posts': today_posts,
                'platform_breakdown': {stat.platform: stat.count for stat in platform_stats},
                'last_collection': last_collection,
                'latest_post_time': latest_post_time,
                'last_updated': datetime.utcnow().isoformat()
            }
            
        except Exception as e:
            logger.error(f"Error getting collection stats: {e}")
            return {
                'error': str(e),
                'last_updated': datetime.utcnow().isoformat()
            }